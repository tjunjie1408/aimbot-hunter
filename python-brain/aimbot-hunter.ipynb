{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1bb97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from google.colab import files\n",
    "\n",
    "TIME_STEPS = 50\n",
    "epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc3149",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_process_data(filepath, scaler=None, fit_scaler=False):\n",
    "    print(f\"Loading {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    df['dx'] = df['x'].diff().fillna(0)\n",
    "    df['dy'] = df['y'].diff().fillna(0)\n",
    "\n",
    "    data = df[['dx', 'dy']].values\n",
    "\n",
    "    if fit_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "    else:\n",
    "        data_scaled = scaler.transform(data)\n",
    "\n",
    "    X = []\n",
    "    for i in range(len(data_scaled) - TIME_STEPS):\n",
    "        X.append(data_scaled[i:(i + TIME_STEPS)])\n",
    "\n",
    "    return np.array(X), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264323ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, scaler = load_and_process_data('/content/golden_human.csv', fit_scaler=True)\n",
    "joblib.dump(scaler, 'scaler.save')\n",
    "print(f\"Human Data Shape: {X_train.shape}\")\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(32, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),\n",
    "    RepeatVector(X_train.shape[1]),\n",
    "    LSTM(32, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(X_train.shape[2]))\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, shuffle=True)\n",
    "model.save('aimbot_hunter_model.h5')\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44769498",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_for_colab(filename, scaler):\n",
    "    path = f\"./{filename}\" \n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df['dx'] = df['x'].diff().fillna(0)\n",
    "        df['dy'] = df['y'].diff().fillna(0)\n",
    "        data = df[['dx', 'dy']].values\n",
    "        data_scaled = scaler.transform(data)\n",
    "        \n",
    "        X = []\n",
    "        for i in range(len(data_scaled) - TIME_STEPS):\n",
    "            X.append(data_scaled[i:(i + TIME_STEPS)])\n",
    "        return np.array(X)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Could not find {filename}, skipping this data...\")\n",
    "        return None\n",
    "\n",
    "def calculate_loss(model, data):\n",
    "    reconstructions = model.predict(data)\n",
    "    return np.mean(np.abs(reconstructions - data), axis=(1, 2))\n",
    "\n",
    "print(\">>> Judging...\")\n",
    "\n",
    "# model = tf.keras.models.load_model('aimbot_hunter_model.h5')\n",
    "# scaler = joblib.load('scaler.save')\n",
    "\n",
    "scores = {}\n",
    "human_X = load_data_for_colab('golden_human.csv', scaler)\n",
    "linear_X = load_data_for_colab('golden_linear.csv', scaler)\n",
    "bezier_X = load_data_for_colab('golden_bezier.csv', scaler)\n",
    "\n",
    "if human_X is not None: scores['Human'] = calculate_loss(model, human_X)\n",
    "if linear_X is not None: scores['Linear Bot'] = calculate_loss(model, linear_X)\n",
    "if bezier_X is not None: scores['Bezier Bot'] = calculate_loss(model, bezier_X)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for label, score in scores.items():\n",
    "    sns.histplot(score, bins=50, alpha=0.6, label=label, kde=True)\n",
    "\n",
    "plt.axvline(x=0.05, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Final Verdict: Human vs Aimbots')\n",
    "plt.xlabel('Reconstruction Error (Loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('final_verdict.png')\n",
    "plt.show()\n",
    "\n",
    "print(\">>> Packing and downloading critical files...\")\n",
    "files.download('aimbot_hunter_model.h5')\n",
    "files.download('scaler.save')\n",
    "files.download('final_verdict.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
